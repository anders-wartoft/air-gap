# Id to use in some logs
id=Gap_Detector_1
# The same server should hold both topics
bootstrapServers=192.168.153.138:9092
# Input topic. The topic downstream writes to
topic=log2
# Output topic. The topic to read from when sending logs to your SIEM
topicOut=dedup2
# More information logged
verbose=true
# The id to use when accessing Kafka
clientId=gap_detector
# How often should we send gap events as log events. Save the state and where we are in Kafka
sendGapsEverySeconds=5
# Start from timestamp. This is set automatically every time the gaps are sent as a log event. 
# When the gaps are sent, the gapSaveFile is updated with the current state of all gaps, and
# this field is updated to the save timestamp. If the application crashes, it will load the
# saved gaps and start to read from the from timestamp. That behaviour should avoid missing 
# events, but may introduce duplicates. For, e.g., Elasticsearch, there are methods to avoid 
# duplicates by setting the key to each event from the event (if it's unique).
# The key in Kafka downstream should be unique, since it's the topic name, partition id and 
# position in the upstream Kafka. Use different topic names if you have several instances of
# the upstream/downstream code.
# If you stop the gap-detector, it will try to save the gap state and then remove the from
# timestamp from this file, since we have a clean shutdown. The timestamp is only needed when
# the application crashes
from=2024-02-03T09:41:03+01:00
# The save file for gaps. It should maybe be monitored for increasing size?
gapSaveFile=/Users/anders/Library/CloudStorage/OneDrive-Privat/transfer/tmp/gaps.json
