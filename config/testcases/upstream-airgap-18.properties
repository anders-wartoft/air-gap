# Used in some log events to identify the source
id=Upstream_17
# Used in the MTU code
nic=lo0
# UDP target (downstream, use static arp and route to be able to send packets over a diode)
#targetIP=kafka-downstream.sitia.nu
targetIP=localhost
# UDP target port
targetPort=1234
# Kafka source. If more than one, separate the servers with a comma ,
source=kafka
bootstrapServers=kafka-upstream.sitia.nu:9094,kafka-upstream.sitia.nu:8094
# Topic to read
topic=transfer
# Kafka group id to use. If several threads are used, this is prepended to the thread names.
groupID=17
logLevel=info
# Read from this time instead of starting at the end
#    2024-01-28T10:24:55+01:00
from=
# For testing, you can use random, else use kafka
#source=random
source=kafka
# Downstream public key file
encryption=false
publicKeyFile=certs/server2.pem
# Every n seconds, generate a new symmetric key
generateNewSymmetricKeyEvery=50
#payloadSize=auto
payloadSize=1500
# After reading the config, where should we send the logs? Default is stdout
#logFileName=./tmp/upstream.log
# Format: {"name": "thread_name", "offset": offset_in_seconds}
#sendingThreads=[{"now": 0}, {"3minutes": -10}]
sendingThreads=[{"No-delay": 0}]
# TLS to Kafka
# Certificate file
certFile=certs/tmp/airgap-upstream.crt
# Key file
keyFile=certs/tmp/airgap-upstream.key.enc
# Password file for the encrypted key file
keyPasswordFile=certs/tmp/airgap-upstream.pw
# CA file
caFile=certs/tmp/kafka-ca.crt

# To test the gap detection removing gaps, remove the filter out for previously dropped packets
# Deliver every even numbered packet:
#deliverFilter=2,4,6
# compress the payload if events longer than this in bytes
compressWhenLengthExceeds=1200

logStatistics=60

eps=500